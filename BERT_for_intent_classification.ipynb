{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_for_intent_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhenwenzhang/BERT-SLU/blob/master/BERT_for_intent_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w83aw3VcFMe",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Driver\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd3Prb2QLZj2",
        "colab_type": "code",
        "outputId": "92341252-aecd-4e87-edd3-deb501d315fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! nvidia-smi"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Mon Jul 22 04:30:08 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtlE7K1NcTK6",
        "colab_type": "text"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:03.192809Z",
          "start_time": "2019-05-10T03:00:01.649485Z"
        },
        "id": "WiZH9TXt2gcs",
        "colab_type": "code",
        "outputId": "5f7dcfae-97fc-4d62-90e4-57252be06a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "\"\"\"BERT finetuning runner.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import csv\n",
        "import os\n",
        "from bert import modeling\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "import tensorflow as tf\n",
        "from sklearn import metrics"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0722 02:55:33.261201 140274862520192 deprecation_wrapper.py:119] From /content/drive/My Drive/MyProjects/BERT-SLU/bert/optimization.py:84: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh_2I89qRIYY",
        "colab_type": "code",
        "outputId": "0f251cb4-63b3-4682-c0dc-95ea00f981fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# only needs to be done once\n",
        "# https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
        "  \n",
        "# https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip\n",
        "# if os.path.exists('uncased_L-12_H-768_A-12.zip'):\n",
        "! wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "! unzip uncased_L-12_H-768_A-12.zip\n",
        "    \n",
        "! wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n",
        "! unzip uncased_L-24_H-1024_A-16.zip\n",
        "\n",
        "! wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
        "! unzip cased_L-12_H-768_A-12.zip\n",
        "\n",
        "! wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip\n",
        "! unzip cased_L-24_H-1024_A-16.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-22 02:55:40--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.76.128, 2a00:1450:400c:c04::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.76.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M  43.5MB/s    in 8.8s    \n",
            "\n",
            "2019-07-22 02:55:49 (44.1 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n",
            "--2019-07-22 02:56:01--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 66.102.1.128, 2a00:1450:400c:c0c::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|66.102.1.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1247797031 (1.2G) [application/zip]\n",
            "Saving to: ‘uncased_L-24_H-1024_A-16.zip’\n",
            "\n",
            "uncased_L-24_H-1024 100%[===================>]   1.16G  22.6MB/s    in 45s     \n",
            "\n",
            "2019-07-22 02:56:47 (26.3 MB/s) - ‘uncased_L-24_H-1024_A-16.zip’ saved [1247797031/1247797031]\n",
            "\n",
            "Archive:  uncased_L-24_H-1024_A-16.zip\n",
            "   creating: uncased_L-24_H-1024_A-16/\n",
            "  inflating: uncased_L-24_H-1024_A-16/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-24_H-1024_A-16/vocab.txt  \n",
            "  inflating: uncased_L-24_H-1024_A-16/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-24_H-1024_A-16/bert_config.json  \n",
            "--2019-07-22 02:57:15--  https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.133.128, 2a00:1450:400c:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.133.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 404261442 (386M) [application/zip]\n",
            "Saving to: ‘cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "cased_L-12_H-768_A- 100%[===================>] 385.53M  20.4MB/s    in 13s     \n",
            "\n",
            "2019-07-22 02:57:29 (29.7 MB/s) - ‘cased_L-12_H-768_A-12.zip’ saved [404261442/404261442]\n",
            "\n",
            "Archive:  cased_L-12_H-768_A-12.zip\n",
            "   creating: cased_L-12_H-768_A-12/\n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_config.json  \n",
            "--2019-07-22 02:57:40--  https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.166.128, 2a00:1450:400c:c0c::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.166.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1242178883 (1.2G) [application/zip]\n",
            "Saving to: ‘cased_L-24_H-1024_A-16.zip’\n",
            "\n",
            "cased_L-24_H-1024_A 100%[===================>]   1.16G  27.8MB/s    in 48s     \n",
            "\n",
            "2019-07-22 02:58:29 (24.5 MB/s) - ‘cased_L-24_H-1024_A-16.zip’ saved [1242178883/1242178883]\n",
            "\n",
            "Archive:  cased_L-24_H-1024_A-16.zip\n",
            "   creating: cased_L-24_H-1024_A-16/\n",
            "  inflating: cased_L-24_H-1024_A-16/bert_model.ckpt.meta  \n",
            "  inflating: cased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: cased_L-24_H-1024_A-16/vocab.txt  \n",
            "  inflating: cased_L-24_H-1024_A-16/bert_model.ckpt.index  \n",
            "  inflating: cased_L-24_H-1024_A-16/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxysA12lcaxW",
        "colab_type": "text"
      },
      "source": [
        "### Parameter Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZk77YJ8Fzvl",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "max_seq_length = 50 #@param {type:\"integer\"}\n",
        "train_batch_size = 32 #@param {type:\"integer\"}\n",
        "eval_batch_size = 32 #@param {type:\"integer\"}\n",
        "predict_batch_size = 32 #@param {type:\"integer\"}\n",
        "\n",
        "warmup_proportion = 0.1\n",
        "save_checkpoints_steps = 1000\n",
        "log_step_count_steps = 10\n",
        "save_summary_steps = 1\n",
        "\n",
        "learning_rate = 5e-5 #@param [\"5e-5\", \"3e-5\", \"2e-5\"] {type:\"raw\"}\n",
        "num_train_epochs = 3 #@param {type:\"integer\",min:1, max:10, step:1}\n",
        "do_train = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "do_eval = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "do_predict = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "\n",
        "log_dir = '/content/log' #@param {type:\"string\"}\n",
        "data_dir = 'data/atis' #@param [\"data/atis\", \"data/snips\"]\n",
        "checkpoints = 'checkpoints/cased_L-12_H-768_A-12' #@param [\"checkpoints/multi_cased_L-12_H-768_A-12\",\"checkpoints/cased_L-12_H-768_A-12\",\"checkpoints/cased_L-24_H-1024_A-16\",\"checkpoints/uncased_L-12_H-768_A-12\",\"checkpoints/uncased_L-24_H-1024_A-16\"]\n",
        "bert_config_file = os.path.join(checkpoints, 'bert_config.json')\n",
        "vocab_file = os.path.join(checkpoints, 'vocab.txt')\n",
        "init_checkpoint = os.path.join(checkpoints, 'bert_model.ckpt')\n",
        "\n",
        "if checkpoints.split('/')[1].startswith('u'):\n",
        "  do_lower_case = True\n",
        "else:\n",
        "  do_lower_case = False    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzGT94z6cqmE",
        "colab_type": "text"
      },
      "source": [
        "### Input Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:03.427032Z",
          "start_time": "2019-05-10T03:00:03.314067Z"
        },
        "id": "OmddfmB_2gcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "          guid: Unique id for the example.\n",
        "          text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "          text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "          label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:03.541764Z",
          "start_time": "2019-05-10T03:00:03.432892Z"
        },
        "id": "ttUP5Rbz2gc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PaddingInputExample(object):\n",
        "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "    When running eval/predict on the TPU, we need to pad the number of examples\n",
        "    to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "    size. The alternative is to drop the last batch, which is bad because it means\n",
        "    the entire output data won't be generated.\n",
        "    We use this class instead of `None` because treating `None` as padding\n",
        "    battches could cause silent errors.\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:03.650346Z",
          "start_time": "2019-05-10T03:00:03.544107Z"
        },
        "id": "bTq05_Jn2gc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_ids,\n",
        "                 input_mask,\n",
        "                 segment_ids,\n",
        "                 label_id,\n",
        "                 is_real_example=True):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "        self.is_real_example = is_real_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6_hPlqxcy_M",
        "colab_type": "text"
      },
      "source": [
        "### Data Processor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:03.760535Z",
          "start_time": "2019-05-10T03:00:03.652960Z"
        },
        "id": "o00iCDi72gc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataProcessor(object):\n",
        "    \"\"\"Processor for the ATIS data set.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.train_path = os.path.join(self.data_dir, \"train.tsv\")\n",
        "        self.dev_path = os.path.join(self.data_dir, \"dev.tsv\")\n",
        "        self.test_path = os.path.join(self.data_dir, \"test.tsv\")\n",
        "\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with tf.gfile.Open(input_file, \"r\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = tokenization.convert_to_unicode(line[1])\n",
        "            label = tokenization.convert_to_unicode(line[0])\n",
        "            examples.append(\n",
        "                InputExample(\n",
        "                    guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "    def get_train_examples(self):\n",
        "        return self._create_examples(self._read_tsv(self.train_path), \"train\")\n",
        "\n",
        "    def get_dev_examples(self):\n",
        "        return self._create_examples(self._read_tsv(self.dev_path), \"dev\")\n",
        "\n",
        "    def get_test_examples(self):\n",
        "        return self._create_examples(self._read_tsv(self.test_path), \"test\")\n",
        "\n",
        "    def get_labels_info(self):\n",
        "        labels = []\n",
        "        label_map = {}\n",
        "        label_map_file = os.path.join(log_dir, \"label_map.txt\")\n",
        "        lines = self._read_tsv(self.train_path) + \\\n",
        "                self._read_tsv(self.dev_path) + \\\n",
        "                self._read_tsv(self.test_path)\n",
        "\n",
        "        for line in lines:\n",
        "            labels += line[0].strip().split()\n",
        "\n",
        "        labels = sorted(set(labels), reverse=True)\n",
        "        num_labels = sorted(set(labels), reverse=True).__len__()\n",
        "\n",
        "        with tf.gfile.GFile(label_map_file, \"w\") as writer:\n",
        "            for (i, label) in enumerate(labels):\n",
        "                label_map[label] = i\n",
        "                writer.write(\"{}:{}\\n\".format(i, label))\n",
        "        return label_map, num_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:03.870000Z",
          "start_time": "2019-05-10T03:00:03.763149Z"
        },
        "id": "OSRm4NKu2gc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_single_example(ex_index, example, label_map, max_seq_length,\n",
        "                           tokenizer):\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        return InputFeatures(\n",
        "            input_ids=[0] * max_seq_length,\n",
        "            input_mask=[0] * max_seq_length,\n",
        "            segment_ids=[0] * max_seq_length,\n",
        "            label_id=0,\n",
        "            is_real_example=False)\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "    tokens_b = None\n",
        "    if example.text_b:\n",
        "        tokens_b = tokenizer.tokenize(example.text_b)\n",
        "\n",
        "    if tokens_b:\n",
        "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "        # length is less than the specified length.\n",
        "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "    else:\n",
        "        # Account for [CLS] and [SEP] with \"- 2\"\n",
        "        if len(tokens_a) > max_seq_length - 2:\n",
        "            tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "\n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    segment_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(0)\n",
        "\n",
        "    if tokens_b:\n",
        "        for token in tokens_b:\n",
        "            tokens.append(token)\n",
        "            segment_ids.append(1)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(1)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    label_id = label_map[example.label]\n",
        "    if ex_index < 2:\n",
        "        tf.logging.info(\"*** Example ***\")\n",
        "        tf.logging.info(\"guid: %s\" % (example.guid))\n",
        "        tf.logging.info(\"tokens: %s\" % \" \".join(\n",
        "            [tokenization.printable_text(x) for x in tokens]))\n",
        "        tf.logging.info(\n",
        "            \"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "        tf.logging.info(\n",
        "            \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "        tf.logging.info(\n",
        "            \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "        tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
        "\n",
        "    feature = InputFeatures(\n",
        "        input_ids=input_ids,\n",
        "        input_mask=input_mask,\n",
        "        segment_ids=segment_ids,\n",
        "        label_id=label_id,\n",
        "        is_real_example=True)\n",
        "    return feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:03.968732Z",
          "start_time": "2019-05-10T03:00:03.872481Z"
        },
        "id": "TP92iXJz2gc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file_based_convert_examples_to_features(examples, label_map, max_seq_length,\n",
        "                                            tokenizer, output_file):\n",
        "    \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
        "\n",
        "    writer = tf.python_io.TFRecordWriter(output_file)\n",
        "\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 10000 == 0:\n",
        "            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "        feature = convert_single_example(ex_index, example, label_map, max_seq_length, tokenizer)\n",
        "\n",
        "        def create_int_feature(values):\n",
        "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
        "            return f\n",
        "\n",
        "        features = collections.OrderedDict()\n",
        "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
        "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
        "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
        "        features[\"label_ids\"] = create_int_feature([feature.label_id])\n",
        "        features[\"is_real_example\"] = create_int_feature([int(feature.is_real_example)])\n",
        "\n",
        "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "    writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:04.077032Z",
          "start_time": "2019-05-10T03:00:03.970740Z"
        },
        "id": "vZ33P5RY2gc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file_based_input_fn_builder(input_file, seq_length, is_training,\n",
        "                                drop_remainder, batch_size):\n",
        "    \"\"\"Creates an `input_fn` closure to be passed to Estimator.\"\"\"\n",
        "\n",
        "    name_to_features = {\n",
        "        \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "        \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "        \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "        \"label_ids\": tf.FixedLenFeature([], tf.int64),\n",
        "        \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
        "    }\n",
        "\n",
        "    def _decode_record(record, name_to_features):\n",
        "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "        example = tf.parse_single_example(record, name_to_features)\n",
        "\n",
        "        return example\n",
        "\n",
        "    def input_fn():\n",
        "        \"\"\"The actual input function.\"\"\"\n",
        "\n",
        "        # For training, we want a lot of parallel reading and shuffling.\n",
        "        # For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "        d = tf.data.TFRecordDataset(input_file)\n",
        "        if is_training:\n",
        "            d = d.repeat()\n",
        "            d = d.shuffle(buffer_size=100)\n",
        "\n",
        "        d = d.apply(\n",
        "            tf.data.experimental.map_and_batch(\n",
        "                lambda record: _decode_record(record, name_to_features),\n",
        "                batch_size=batch_size,\n",
        "                drop_remainder=drop_remainder))\n",
        "        return d\n",
        "\n",
        "    return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:04.185478Z",
          "start_time": "2019-05-10T03:00:04.079329Z"
        },
        "id": "vP6nc7xq2gdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3suvDKrc9_o",
        "colab_type": "text"
      },
      "source": [
        "### Model Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:04.294358Z",
          "start_time": "2019-05-10T03:00:04.187995Z"
        },
        "id": "G7-9pHB02gdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels):\n",
        "    model = modeling.BertModel(\n",
        "        config=bert_config,\n",
        "        is_training=is_training,\n",
        "        input_ids=input_ids,\n",
        "        input_mask=input_mask,\n",
        "        token_type_ids=segment_ids)\n",
        "\n",
        "    # If you want to use the token-level output, use model.get_sequence_output()\n",
        "    # instead.\n",
        "    output_layer = model.get_pooled_output()\n",
        "\n",
        "    hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "    output_weights = tf.get_variable(\n",
        "        \"output_weights\", [num_labels, hidden_size],\n",
        "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "    output_bias = tf.get_variable(\n",
        "        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "    if is_training:\n",
        "        # I.e., 0.1 dropout\n",
        "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, logits, probabilities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:04.404672Z",
          "start_time": "2019-05-10T03:00:04.296622Z"
        },
        "id": "z-RRG_AA2gdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate, num_train_steps, num_warmup_steps):\n",
        "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "\n",
        "        tf.logging.info(\"*** Features ***\")\n",
        "        for name in sorted(features.keys()):\n",
        "            tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "        input_ids = features[\"input_ids\"]\n",
        "        input_mask = features[\"input_mask\"]\n",
        "        segment_ids = features[\"segment_ids\"]\n",
        "        label_ids = features[\"label_ids\"]\n",
        "\n",
        "        is_real_example = None\n",
        "        if \"is_real_example\" in features:\n",
        "            is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
        "        else:\n",
        "            is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
        "\n",
        "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "        (total_loss, logits, probabilities) = create_model(\n",
        "            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "            num_labels)\n",
        "\n",
        "        predicted_class = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "\n",
        "        accuracy = tf.metrics.accuracy(labels=label_ids,\n",
        "                                       predictions=predicted_class,\n",
        "                                       weights=is_real_example, name=\"acc_op\")\n",
        "        tf.summary.scalar(\"accuracy\", accuracy[1])\n",
        "\n",
        "        tvars = tf.trainable_variables()\n",
        "        initialized_variable_names = {}\n",
        "        if init_checkpoint:\n",
        "            (assignment_map, initialized_variable_names\n",
        "             ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "\n",
        "            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "        tf.logging.info(\"**** Trainable Variables ****\")\n",
        "        for var in tvars:\n",
        "            init_string = \"\"\n",
        "            if var.name in initialized_variable_names:\n",
        "                init_string = \", *INIT_FROM_CKPT*\"\n",
        "            tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape, init_string)\n",
        "\n",
        "        output_spec = None\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "            train_op = optimization.create_optimizer(\n",
        "                total_loss, learning_rate, num_train_steps, num_warmup_steps)\n",
        "\n",
        "            output_spec = tf.estimator.EstimatorSpec(\n",
        "                mode=mode,\n",
        "                loss=total_loss,\n",
        "                train_op=train_op\n",
        "            )\n",
        "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "            output_spec = tf.estimator.EstimatorSpec(\n",
        "                mode=mode,\n",
        "                loss=total_loss,\n",
        "                eval_metric_ops={\"accuracy\": accuracy}\n",
        "            )\n",
        "        else:\n",
        "            output_spec = tf.estimator.EstimatorSpec(\n",
        "                mode=mode,\n",
        "                predictions={\"predicted_class\": predicted_class,\n",
        "                             \"true_class\": label_ids}\n",
        "            )\n",
        "        return output_spec\n",
        "\n",
        "    return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixu2NqJIdFbT",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:15:45.304338Z",
          "start_time": "2019-05-10T03:00:04.407792Z"
        },
        "scrolled": true,
        "id": "HxnKgYie2gdH",
        "colab_type": "code",
        "outputId": "e6b12db9-c6bf-4724-bb3c-0f57449caec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "if not do_train and not do_eval and not do_predict:\n",
        "    raise ValueError(\n",
        "        \"At least one of `do_train`, `do_eval` or `do_predict' must be True.\")\n",
        "tf.gfile.MakeDirs(log_dir)\n",
        "processor = DataProcessor(data_dir)\n",
        "label_map, num_labels = processor.get_labels_info()\n",
        "tokenization.validate_case_matches_checkpoint(do_lower_case, init_checkpoint)\n",
        "bert_config = modeling.BertConfig.from_json_file(bert_config_file)\n",
        "\n",
        "if max_seq_length > bert_config.max_position_embeddings:\n",
        "    raise ValueError(\"Cannot use sequence length %d because the BERT model \"\n",
        "                     \"was only trained up to sequence length %d\" %\n",
        "                     (max_seq_length, bert_config.max_position_embeddings))\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "    vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=log_dir,\n",
        "    session_config=config,\n",
        "    save_checkpoints_steps=save_checkpoints_steps,\n",
        "    log_step_count_steps=log_step_count_steps,\n",
        "    save_summary_steps=save_summary_steps)\n",
        "train_examples = None\n",
        "num_train_steps = None\n",
        "num_warmup_steps = None\n",
        "\n",
        "if do_train:\n",
        "    train_examples = processor.get_train_examples()\n",
        "    num_train_steps = int(\n",
        "        len(train_examples) / train_batch_size * num_train_epochs)\n",
        "    num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "model_fn = model_fn_builder(\n",
        "    bert_config=bert_config,\n",
        "    num_labels=num_labels,\n",
        "    init_checkpoint=init_checkpoint,\n",
        "    learning_rate=learning_rate,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)\n",
        "\n",
        "# Training\n",
        "if do_train:\n",
        "    train_file = os.path.join(log_dir, \"train.tf_record\")\n",
        "    file_based_convert_examples_to_features(\n",
        "        train_examples, label_map, max_seq_length, tokenizer, train_file)\n",
        "    tf.logging.info(\"***** Running training *****\")\n",
        "    tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
        "    tf.logging.info(\"  Batch size = %d\", train_batch_size)\n",
        "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "    train_input_fn = file_based_input_fn_builder(\n",
        "        input_file=train_file,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=True,\n",
        "        drop_remainder=False,\n",
        "        batch_size=train_batch_size)\n",
        "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0722 03:00:32.357952 140274862520192 deprecation_wrapper.py:119] From /content/drive/My Drive/MyProjects/BERT-SLU/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-743e11141635>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_labels_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_case_matches_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mbert_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBertConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbert_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_position_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/MyProjects/BERT-SLU/bert/modeling.py\u001b[0m in \u001b[0;36mfrom_json_file\u001b[0;34m(cls, json_file)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m       \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mregular\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \"\"\"\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                                            \"File isn't open for reading\")\n\u001b[1;32m     83\u001b[0m       self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\n\u001b[0;32m---> 84\u001b[0;31m           compat.as_bytes(self.__name), 1024 * 512)\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: checkpoints/cased_L-12_H-768_A-12/bert_config.json; No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYRHXA2cdKyk",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:15:45.309668Z",
          "start_time": "2019-05-10T03:00:04.197Z"
        },
        "id": "9m6A-f2V2gdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if do_eval:\n",
        "    eval_examples = processor.get_dev_examples()\n",
        "    num_actual_eval_examples = len(eval_examples)\n",
        "    eval_file = os.path.join(log_dir, \"eval.tf_record\")\n",
        "    file_based_convert_examples_to_features(\n",
        "        eval_examples, label_map, max_seq_length, tokenizer, eval_file)\n",
        "    tf.logging.info(\"***** Running evaluation *****\")\n",
        "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
        "                    len(eval_examples), num_actual_eval_examples,\n",
        "                    len(eval_examples) - num_actual_eval_examples)\n",
        "    tf.logging.info(\"  Batch size = %d\", eval_batch_size)\n",
        "\n",
        "    eval_input_fn = file_based_input_fn_builder(\n",
        "        input_file=eval_file,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=False,\n",
        "        drop_remainder=False,\n",
        "        batch_size=eval_batch_size)\n",
        "\n",
        "    result = estimator.evaluate(input_fn=eval_input_fn)\n",
        "\n",
        "    output_eval_file = os.path.join(log_dir, \"eval_results.txt\")\n",
        "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "        tf.logging.info(\"***** Eval results *****\")\n",
        "        for key in sorted(result.keys()):\n",
        "            tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKalVYoAdOno",
        "colab_type": "text"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:15:45.310973Z",
          "start_time": "2019-05-10T03:00:04.199Z"
        },
        "id": "8MCEcX892gdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if do_predict:\n",
        "\n",
        "    predict_examples = processor.get_test_examples()\n",
        "    num_actual_predict_examples = len(predict_examples)\n",
        "\n",
        "    predict_file = os.path.join(log_dir, \"predict.tf_record\")\n",
        "    if not tf.gfile.Exists(predict_file):\n",
        "        file_based_convert_examples_to_features(predict_examples, label_map,\n",
        "                                                max_seq_length, tokenizer,\n",
        "                                                predict_file)\n",
        "\n",
        "    tf.logging.info(\"***** Running prediction*****\")\n",
        "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
        "                    len(predict_examples), num_actual_predict_examples,\n",
        "                    len(predict_examples) - num_actual_predict_examples)\n",
        "    tf.logging.info(\"  Batch size = %d\", predict_batch_size)\n",
        "\n",
        "    predict_input_fn = file_based_input_fn_builder(\n",
        "        input_file=predict_file,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=False,\n",
        "        drop_remainder=False,\n",
        "        batch_size=predict_batch_size)\n",
        "\n",
        "    result = estimator.predict(input_fn=predict_input_fn)\n",
        "    label_map_new = {v: k for k, v in label_map.items()}\n",
        "\n",
        "\n",
        "    output_predict_file = os.path.join(log_dir, \"test_results.tsv\")\n",
        "    with tf.gfile.GFile(output_predict_file, \"w\") as writer:\n",
        "        writer.write(\"line, true, predict \\n\")\n",
        "        tf.logging.info(\"***** Predict results *****\")\n",
        "\n",
        "        true_classes = []\n",
        "        predicted_classes = []\n",
        "\n",
        "        for i, item in enumerate(result):\n",
        "                true_class = item[\"true_class\"]\n",
        "                predicted_class = item[\"predicted_class\"]\n",
        "\n",
        "                true_classes.append(true_class)\n",
        "                predicted_classes.append(predicted_class)\n",
        "\n",
        "                if predicted_class != true_class:\n",
        "                    output_line = \"{}, {}, {}\\n\".format(i + 1, label_map_new[true_class],\n",
        "                                                        label_map_new[predicted_class])\n",
        "                    writer.write(output_line)\n",
        "    accuracy = metrics.accuracy_score(true_classes, predicted_classes)\n",
        "#     classification_report = metrics.classification_report(true_classes, predicted_classes)\n",
        "    tf.logging.info(\"Accuracy: %s\", accuracy)\n",
        "#     tf.logging.info(\"Accuracy: %s\", classification_report)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}