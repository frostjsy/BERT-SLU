{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BERT_for_intent_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhenwenzhang/BERT-SLU/blob/master/BERT_for_intent_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtlE7K1NcTK6",
        "colab_type": "text"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXbuDbK85HIy",
        "colab_type": "code",
        "outputId": "2381364e-eb3e-4d1e-c74a-b913496d5c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "% cd BERT-SLU\n",
        "! rm -rf log"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/BERT-SLU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:03.192809Z",
          "start_time": "2019-05-10T03:00:01.649485Z"
        },
        "id": "WiZH9TXt2gcs",
        "colab_type": "code",
        "outputId": "3e851419-92ab-4389-c2e1-06cba592c664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "\"\"\"BERT finetuning runner.\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import csv\n",
        "import os\n",
        "from bert import modeling\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "import tensorflow as tf\n",
        "from sklearn import metrics"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0723 13:17:26.078833 140336914397056 deprecation_wrapper.py:119] From /content/BERT-SLU/bert/optimization.py:84: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh_2I89qRIYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download pre-trained models\n",
        "# ! wget -P checkpoints https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
        "# ! wget -P checkpoints https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip\n",
        "# ! wget -P checkpoints https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip \n",
        "# ! wget -P checkpoints https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n",
        "# ! wget -P checkpoints https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
        "# ! wget -P checkpoints https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip\n",
        "\n",
        "# ! unzip -o checkpoints/multi_cased_L-12_H-768_A-12.zip -d checkpoints\n",
        "# ! unzip -o checkpoints/multilingual_L-12_H-768_A-12.zip -d checkpoints\n",
        "# ! unzip -o checkpoints/uncased_L-12_H-768_A-12.zip -d checkpoints\n",
        "# ! unzip -o checkpoints/uncased_L-24_H-1024_A-16.zip -d checkpoints\n",
        "# ! unzip -o checkpoints/cased_L-12_H-768_A-12.zip -d checkpoints\n",
        "# ! unzip -o checkpoints/cased_L-24_H-1024_A-16.zip -d checkpoints\n",
        "\n",
        "# ! rm -rf *.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxysA12lcaxW",
        "colab_type": "text"
      },
      "source": [
        "### Parameter Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZk77YJ8Fzvl",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "max_seq_length = 50 #@param {type:\"integer\"}\n",
        "train_batch_size = 32 #@param {type:\"integer\"}\n",
        "eval_batch_size = 32 #@param {type:\"integer\"}\n",
        "predict_batch_size = 32 #@param {type:\"integer\"}\n",
        "\n",
        "warmup_proportion = 0.1\n",
        "save_checkpoints_steps = 1000\n",
        "log_step_count_steps = 10\n",
        "save_summary_steps = 1\n",
        "\n",
        "learning_rate = 5e-5 #@param [\"5e-5\", \"3e-5\", \"2e-5\"] {type:\"raw\"}\n",
        "num_train_epochs = 3 #@param {type:\"integer\",min:1, max:10, step:1}\n",
        "do_train = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "do_eval = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "do_predict = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "\n",
        "log_dir = 'log' #@param {type:\"string\"}\n",
        "data_dir = 'data/atis' #@param [\"data/atis\", \"data/snips\"]\n",
        "checkpoints = 'checkpoints/multi_cased_L-12_H-768_A-12' #@param [\"checkpoints/multi_cased_L-12_H-768_A-12\",\"checkpoints/cased_L-12_H-768_A-12\",\"checkpoints/cased_L-24_H-1024_A-16\",\"checkpoints/uncased_L-12_H-768_A-12\",\"checkpoints/uncased_L-24_H-1024_A-16\"]\n",
        "bert_config_file = os.path.join(checkpoints, 'bert_config.json')\n",
        "vocab_file = os.path.join(checkpoints, 'vocab.txt')\n",
        "init_checkpoint = os.path.join(checkpoints, 'bert_model.ckpt')\n",
        "\n",
        "if checkpoints.split('/')[1].startswith('u'):\n",
        "  do_lower_case = True\n",
        "else:\n",
        "  do_lower_case = False    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzGT94z6cqmE",
        "colab_type": "text"
      },
      "source": [
        "### Input Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:03.427032Z",
          "start_time": "2019-05-10T03:00:03.314067Z"
        },
        "id": "OmddfmB_2gcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "          guid: Unique id for the example.\n",
        "          text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "          text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "          label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:03.541764Z",
          "start_time": "2019-05-10T03:00:03.432892Z"
        },
        "id": "ttUP5Rbz2gc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PaddingInputExample(object):\n",
        "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "    When running eval/predict on the TPU, we need to pad the number of examples\n",
        "    to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "    size. The alternative is to drop the last batch, which is bad because it means\n",
        "    the entire output data won't be generated.\n",
        "    We use this class instead of `None` because treating `None` as padding\n",
        "    battches could cause silent errors.\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:03.650346Z",
          "start_time": "2019-05-10T03:00:03.544107Z"
        },
        "id": "bTq05_Jn2gc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_ids,\n",
        "                 input_mask,\n",
        "                 segment_ids,\n",
        "                 label_id,\n",
        "                 is_real_example=True):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "        self.is_real_example = is_real_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6_hPlqxcy_M",
        "colab_type": "text"
      },
      "source": [
        "### Data Processor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:03.760535Z",
          "start_time": "2019-05-10T03:00:03.652960Z"
        },
        "id": "o00iCDi72gc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataProcessor(object):\n",
        "    \"\"\"Processor for the ATIS data set.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.train_path = os.path.join(self.data_dir, \"train.tsv\")\n",
        "        self.dev_path = os.path.join(self.data_dir, \"dev.tsv\")\n",
        "        self.test_path = os.path.join(self.data_dir, \"test.tsv\")\n",
        "\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with tf.gfile.Open(input_file, \"r\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = tokenization.convert_to_unicode(line[1])\n",
        "            label = tokenization.convert_to_unicode(line[0])\n",
        "            examples.append(\n",
        "                InputExample(\n",
        "                    guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "    def get_train_examples(self):\n",
        "        return self._create_examples(self._read_tsv(self.train_path), \"train\")\n",
        "\n",
        "    def get_dev_examples(self):\n",
        "        return self._create_examples(self._read_tsv(self.dev_path), \"dev\")\n",
        "\n",
        "    def get_test_examples(self):\n",
        "        return self._create_examples(self._read_tsv(self.test_path), \"test\")\n",
        "\n",
        "    def get_labels_info(self):\n",
        "        labels = []\n",
        "        label_map = {}\n",
        "        label_map_file = os.path.join(log_dir, \"label_map.txt\")\n",
        "        lines = self._read_tsv(self.train_path) + \\\n",
        "                self._read_tsv(self.dev_path) + \\\n",
        "                self._read_tsv(self.test_path)\n",
        "\n",
        "        for line in lines:\n",
        "            labels += line[0].strip().split()\n",
        "\n",
        "        labels = sorted(set(labels), reverse=True)\n",
        "        num_labels = sorted(set(labels), reverse=True).__len__()\n",
        "\n",
        "        with tf.gfile.GFile(label_map_file, \"w\") as writer:\n",
        "            for (i, label) in enumerate(labels):\n",
        "                label_map[label] = i\n",
        "                writer.write(\"{}:{}\\n\".format(i, label))\n",
        "        return label_map, num_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:03.870000Z",
          "start_time": "2019-05-10T03:00:03.763149Z"
        },
        "id": "OSRm4NKu2gc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_single_example(ex_index, example, label_map, max_seq_length,\n",
        "                           tokenizer):\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        return InputFeatures(\n",
        "            input_ids=[0] * max_seq_length,\n",
        "            input_mask=[0] * max_seq_length,\n",
        "            segment_ids=[0] * max_seq_length,\n",
        "            label_id=0,\n",
        "            is_real_example=False)\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "    tokens_b = None\n",
        "    if example.text_b:\n",
        "        tokens_b = tokenizer.tokenize(example.text_b)\n",
        "\n",
        "    if tokens_b:\n",
        "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "        # length is less than the specified length.\n",
        "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "    else:\n",
        "        # Account for [CLS] and [SEP] with \"- 2\"\n",
        "        if len(tokens_a) > max_seq_length - 2:\n",
        "            tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "\n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    segment_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(0)\n",
        "\n",
        "    if tokens_b:\n",
        "        for token in tokens_b:\n",
        "            tokens.append(token)\n",
        "            segment_ids.append(1)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(1)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    label_id = label_map[example.label]\n",
        "    if ex_index < 2:\n",
        "        tf.logging.info(\"*** Example ***\")\n",
        "        tf.logging.info(\"guid: %s\" % (example.guid))\n",
        "        tf.logging.info(\"tokens: %s\" % \" \".join(\n",
        "            [tokenization.printable_text(x) for x in tokens]))\n",
        "        tf.logging.info(\n",
        "            \"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "        tf.logging.info(\n",
        "            \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "        tf.logging.info(\n",
        "            \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "        tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
        "\n",
        "    feature = InputFeatures(\n",
        "        input_ids=input_ids,\n",
        "        input_mask=input_mask,\n",
        "        segment_ids=segment_ids,\n",
        "        label_id=label_id,\n",
        "        is_real_example=True)\n",
        "    return feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:03.968732Z",
          "start_time": "2019-05-10T03:00:03.872481Z"
        },
        "id": "TP92iXJz2gc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file_based_convert_examples_to_features(examples, label_map, max_seq_length,\n",
        "                                            tokenizer, output_file):\n",
        "    \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
        "\n",
        "    writer = tf.python_io.TFRecordWriter(output_file)\n",
        "\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 10000 == 0:\n",
        "            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "        feature = convert_single_example(ex_index, example, label_map, max_seq_length, tokenizer)\n",
        "\n",
        "        def create_int_feature(values):\n",
        "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
        "            return f\n",
        "\n",
        "        features = collections.OrderedDict()\n",
        "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
        "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
        "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
        "        features[\"label_ids\"] = create_int_feature([feature.label_id])\n",
        "        features[\"is_real_example\"] = create_int_feature([int(feature.is_real_example)])\n",
        "\n",
        "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "    writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:04.077032Z",
          "start_time": "2019-05-10T03:00:03.970740Z"
        },
        "id": "vZ33P5RY2gc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file_based_input_fn_builder(input_file, seq_length, is_training,\n",
        "                                drop_remainder, batch_size):\n",
        "    \"\"\"Creates an `input_fn` closure to be passed to Estimator.\"\"\"\n",
        "\n",
        "    name_to_features = {\n",
        "        \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "        \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "        \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "        \"label_ids\": tf.FixedLenFeature([], tf.int64),\n",
        "        \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
        "    }\n",
        "\n",
        "    def _decode_record(record, name_to_features):\n",
        "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "        example = tf.parse_single_example(record, name_to_features)\n",
        "\n",
        "        return example\n",
        "\n",
        "    def input_fn():\n",
        "        \"\"\"The actual input function.\"\"\"\n",
        "\n",
        "        # For training, we want a lot of parallel reading and shuffling.\n",
        "        # For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "        d = tf.data.TFRecordDataset(input_file)\n",
        "        if is_training:\n",
        "            d = d.repeat()\n",
        "            d = d.shuffle(buffer_size=100)\n",
        "\n",
        "        d = d.apply(\n",
        "            tf.data.experimental.map_and_batch(\n",
        "                lambda record: _decode_record(record, name_to_features),\n",
        "                batch_size=batch_size,\n",
        "                drop_remainder=drop_remainder))\n",
        "        return d\n",
        "\n",
        "    return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:04.185478Z",
          "start_time": "2019-05-10T03:00:04.079329Z"
        },
        "id": "vP6nc7xq2gdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3suvDKrc9_o",
        "colab_type": "text"
      },
      "source": [
        "### Model Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:04.294358Z",
          "start_time": "2019-05-10T03:00:04.187995Z"
        },
        "id": "G7-9pHB02gdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels):\n",
        "    model = modeling.BertModel(\n",
        "        config=bert_config,\n",
        "        is_training=is_training,\n",
        "        input_ids=input_ids,\n",
        "        input_mask=input_mask,\n",
        "        token_type_ids=segment_ids)\n",
        "\n",
        "    # If you want to use the token-level output, use model.get_sequence_output()\n",
        "    # instead.\n",
        "    output_layer = model.get_pooled_output()\n",
        "\n",
        "    hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "    output_weights = tf.get_variable(\n",
        "        \"output_weights\", [num_labels, hidden_size],\n",
        "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "    output_bias = tf.get_variable(\n",
        "        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "    if is_training:\n",
        "        # I.e., 0.1 dropout\n",
        "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, logits, probabilities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:00:04.404672Z",
          "start_time": "2019-05-10T03:00:04.296622Z"
        },
        "id": "z-RRG_AA2gdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate, num_train_steps, num_warmup_steps):\n",
        "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "\n",
        "        tf.logging.info(\"*** Features ***\")\n",
        "        for name in sorted(features.keys()):\n",
        "            tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "        input_ids = features[\"input_ids\"]\n",
        "        input_mask = features[\"input_mask\"]\n",
        "        segment_ids = features[\"segment_ids\"]\n",
        "        label_ids = features[\"label_ids\"]\n",
        "\n",
        "        is_real_example = None\n",
        "        if \"is_real_example\" in features:\n",
        "            is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
        "        else:\n",
        "            is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
        "\n",
        "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "        (total_loss, logits, probabilities) = create_model(\n",
        "            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "            num_labels)\n",
        "\n",
        "        predicted_class = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "\n",
        "        accuracy = tf.metrics.accuracy(labels=label_ids,\n",
        "                                       predictions=predicted_class,\n",
        "                                       weights=is_real_example, name=\"acc_op\")\n",
        "        tf.summary.scalar(\"accuracy\", accuracy[1])\n",
        "\n",
        "        tvars = tf.trainable_variables()\n",
        "        initialized_variable_names = {}\n",
        "        if init_checkpoint:\n",
        "            (assignment_map, initialized_variable_names\n",
        "             ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "\n",
        "            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "#         tf.logging.info(\"**** Trainable Variables ****\")\n",
        "#         for var in tvars:\n",
        "#             init_string = \"\"\n",
        "#             if var.name in initialized_variable_names:\n",
        "#                 init_string = \", *INIT_FROM_CKPT*\"\n",
        "#             tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape, init_string)\n",
        "\n",
        "        output_spec = None\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "            train_op = optimization.create_optimizer(\n",
        "                total_loss, learning_rate, num_train_steps, num_warmup_steps)\n",
        "\n",
        "            output_spec = tf.estimator.EstimatorSpec(\n",
        "                mode=mode,\n",
        "                loss=total_loss,\n",
        "                train_op=train_op\n",
        "            )\n",
        "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "            output_spec = tf.estimator.EstimatorSpec(\n",
        "                mode=mode,\n",
        "                loss=total_loss,\n",
        "                eval_metric_ops={\"accuracy\": accuracy}\n",
        "            )\n",
        "        else:\n",
        "            output_spec = tf.estimator.EstimatorSpec(\n",
        "                mode=mode,\n",
        "                predictions={\"predicted_class\": predicted_class,\n",
        "                             \"true_class\": label_ids}\n",
        "            )\n",
        "        return output_spec\n",
        "\n",
        "    return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixu2NqJIdFbT",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:15:45.304338Z",
          "start_time": "2019-05-10T03:00:04.407792Z"
        },
        "scrolled": true,
        "id": "HxnKgYie2gdH",
        "colab_type": "code",
        "outputId": "68b6d82e-0014-4368-ec85-d543cfcb0208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "if not do_train and not do_eval and not do_predict:\n",
        "    raise ValueError(\n",
        "        \"At least one of `do_train`, `do_eval` or `do_predict' must be True.\")\n",
        "tf.gfile.MakeDirs(log_dir)\n",
        "processor = DataProcessor(data_dir)\n",
        "label_map, num_labels = processor.get_labels_info()\n",
        "tokenization.validate_case_matches_checkpoint(do_lower_case, init_checkpoint)\n",
        "bert_config = modeling.BertConfig.from_json_file(bert_config_file)\n",
        "\n",
        "if max_seq_length > bert_config.max_position_embeddings:\n",
        "    raise ValueError(\"Cannot use sequence length %d because the BERT model \"\n",
        "                     \"was only trained up to sequence length %d\" %\n",
        "                     (max_seq_length, bert_config.max_position_embeddings))\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "    vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=log_dir,\n",
        "    session_config=config,\n",
        "    save_checkpoints_steps=save_checkpoints_steps,\n",
        "    log_step_count_steps=log_step_count_steps,\n",
        "    save_summary_steps=save_summary_steps)\n",
        "train_examples = None\n",
        "num_train_steps = None\n",
        "num_warmup_steps = None\n",
        "\n",
        "if do_train:\n",
        "    train_examples = processor.get_train_examples()\n",
        "    num_train_steps = int(\n",
        "        len(train_examples) / train_batch_size * num_train_epochs)\n",
        "    num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "model_fn = model_fn_builder(\n",
        "    bert_config=bert_config,\n",
        "    num_labels=num_labels,\n",
        "    init_checkpoint=init_checkpoint,\n",
        "    learning_rate=learning_rate,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)\n",
        "\n",
        "# Training\n",
        "if do_train:\n",
        "    train_file = os.path.join(log_dir, \"train.tf_record\")\n",
        "    file_based_convert_examples_to_features(\n",
        "        train_examples, label_map, max_seq_length, tokenizer, train_file)\n",
        "    tf.logging.info(\"***** Running training *****\")\n",
        "    tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
        "    tf.logging.info(\"  Batch size = %d\", train_batch_size)\n",
        "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "    train_input_fn = file_based_input_fn_builder(\n",
        "        input_file=train_file,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=True,\n",
        "        drop_remainder=False,\n",
        "        batch_size=train_batch_size)\n",
        "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0723 13:17:26.923466 140336914397056 deprecation_wrapper.py:119] From /content/BERT-SLU/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "I0723 13:17:27.422967 140336914397056 estimator.py:209] Using config: {'_model_dir': 'log', '_tf_random_seed': None, '_save_summary_steps': 1, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': gpu_options {\n",
            "  allow_growth: true\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa267f4aeb8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "W0723 13:17:27.425220 140336914397056 model_fn.py:630] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fa268e112f0>) includes params argument, but params are not passed to Estimator.\n",
            "I0723 13:17:27.428827 140336914397056 <ipython-input-10-4818e34a2ae0>:9] Writing example 0 of 4478\n",
            "I0723 13:17:27.430526 140336914397056 <ipython-input-9-319d0dfaaa64>:63] *** Example ***\n",
            "I0723 13:17:27.431525 140336914397056 <ipython-input-9-319d0dfaaa64>:64] guid: train-0\n",
            "I0723 13:17:27.432383 140336914397056 <ipython-input-9-319d0dfaaa64>:66] tokens: [CLS] i want to fly from bal ##timo ##re to dalla ##s round trip [SEP]\n",
            "I0723 13:17:27.435076 140336914397056 <ipython-input-9-319d0dfaaa64>:68] input_ids: 101 177 21528 10114 26155 10188 20873 65258 10246 10114 11353 10107 13569 37307 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:17:27.436848 140336914397056 <ipython-input-9-319d0dfaaa64>:70] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:17:27.437726 140336914397056 <ipython-input-9-319d0dfaaa64>:72] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:17:27.438627 140336914397056 <ipython-input-9-319d0dfaaa64>:73] label: atis_flight (id = 11)\n",
            "I0723 13:17:27.441925 140336914397056 <ipython-input-9-319d0dfaaa64>:63] *** Example ***\n",
            "I0723 13:17:27.442663 140336914397056 <ipython-input-9-319d0dfaaa64>:64] guid: train-1\n",
            "I0723 13:17:27.443860 140336914397056 <ipython-input-9-319d0dfaaa64>:66] tokens: [CLS] round trip fare ##s from bal ##timo ##re to phi ##lade ##lp ##hia less than 1000 dollars round trip fare ##s from den ##ver to phi ##lade ##lp ##hia less than 1000 dollars round trip fare ##s from pit ##ts ##burgh to phi ##lade ##lp ##hia less than [SEP]\n",
            "I0723 13:17:27.444872 140336914397056 <ipython-input-9-319d0dfaaa64>:68] input_ids: 101 13569 37307 23252 10107 10188 20873 65258 10246 10114 36500 21805 35451 27919 15306 11084 12186 27953 13569 37307 23252 10107 10188 10140 12563 10114 36500 21805 35451 27919 15306 11084 12186 27953 13569 37307 23252 10107 10188 55277 10806 94202 10114 36500 21805 35451 27919 15306 11084 102\n",
            "I0723 13:17:27.445893 140336914397056 <ipython-input-9-319d0dfaaa64>:70] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0723 13:17:27.447061 140336914397056 <ipython-input-9-319d0dfaaa64>:72] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:17:27.448606 140336914397056 <ipython-input-9-319d0dfaaa64>:73] label: atis_airfare (id = 22)\n",
            "I0723 13:17:29.224338 140336914397056 <ipython-input-15-743e11141635>:51] ***** Running training *****\n",
            "I0723 13:17:29.225583 140336914397056 <ipython-input-15-743e11141635>:52]   Num examples = 4478\n",
            "I0723 13:17:29.227239 140336914397056 <ipython-input-15-743e11141635>:53]   Batch size = 32\n",
            "I0723 13:17:29.228408 140336914397056 <ipython-input-15-743e11141635>:54]   Num steps = 419\n",
            "W0723 13:17:29.256868 140336914397056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0723 13:17:29.291377 140336914397056 deprecation.py:323] From <ipython-input-11-a0b5cd77e850>:33: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "I0723 13:17:29.316572 140336914397056 estimator.py:1145] Calling model_fn.\n",
            "I0723 13:17:29.317565 140336914397056 <ipython-input-14-64f48bfe5f20>:4] *** Features ***\n",
            "I0723 13:17:29.318706 140336914397056 <ipython-input-14-64f48bfe5f20>:6]   name = input_ids, shape = (?, 50)\n",
            "I0723 13:17:29.319914 140336914397056 <ipython-input-14-64f48bfe5f20>:6]   name = input_mask, shape = (?, 50)\n",
            "I0723 13:17:29.320904 140336914397056 <ipython-input-14-64f48bfe5f20>:6]   name = is_real_example, shape = (?,)\n",
            "I0723 13:17:29.322414 140336914397056 <ipython-input-14-64f48bfe5f20>:6]   name = label_ids, shape = (?,)\n",
            "I0723 13:17:29.323558 140336914397056 <ipython-input-14-64f48bfe5f20>:6]   name = segment_ids, shape = (?, 50)\n",
            "W0723 13:17:29.331458 140336914397056 deprecation_wrapper.py:119] From /content/BERT-SLU/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0723 13:17:29.335113 140336914397056 deprecation_wrapper.py:119] From /content/BERT-SLU/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0723 13:17:29.380954 140336914397056 deprecation_wrapper.py:119] From /content/BERT-SLU/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0723 13:17:30.013037 140336914397056 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0723 13:17:30.037172 140336914397056 deprecation.py:506] From /content/BERT-SLU/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0723 13:17:30.079021 140336914397056 deprecation.py:323] From /content/BERT-SLU/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0723 13:17:33.821408 140336914397056 deprecation_wrapper.py:119] From /content/BERT-SLU/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0723 13:17:33.823376 140336914397056 deprecation_wrapper.py:119] From /content/BERT-SLU/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0723 13:17:33.835441 140336914397056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0723 13:17:33.854410 140336914397056 deprecation_wrapper.py:119] From /content/BERT-SLU/bert/optimization.py:67: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "W0723 13:17:34.256537 140336914397056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "I0723 13:17:43.138293 140336914397056 estimator.py:1147] Done calling model_fn.\n",
            "I0723 13:17:43.141296 140336914397056 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0723 13:17:46.444327 140336914397056 monitored_session.py:240] Graph was finalized.\n",
            "I0723 13:18:06.810027 140336914397056 session_manager.py:500] Running local_init_op.\n",
            "I0723 13:18:06.982658 140336914397056 session_manager.py:502] Done running local_init_op.\n",
            "I0723 13:18:14.649361 140336914397056 basic_session_run_hooks.py:606] Saving checkpoints for 0 into log/model.ckpt.\n",
            "I0723 13:18:38.518755 140336914397056 basic_session_run_hooks.py:262] loss = 3.2336307, step = 0\n",
            "I0723 13:18:50.276535 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 0.850447\n",
            "I0723 13:18:50.282103 140336914397056 basic_session_run_hooks.py:260] loss = 1.6569368, step = 10 (11.762 sec)\n",
            "I0723 13:18:54.716531 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.25225\n",
            "I0723 13:18:54.722321 140336914397056 basic_session_run_hooks.py:260] loss = 1.5014006, step = 20 (4.441 sec)\n",
            "I0723 13:18:59.180153 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.24035\n",
            "I0723 13:18:59.183539 140336914397056 basic_session_run_hooks.py:260] loss = 0.9081295, step = 30 (4.461 sec)\n",
            "I0723 13:19:03.676866 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.22384\n",
            "I0723 13:19:03.683400 140336914397056 basic_session_run_hooks.py:260] loss = 0.5638126, step = 40 (4.500 sec)\n",
            "I0723 13:19:08.204188 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.20882\n",
            "I0723 13:19:08.207448 140336914397056 basic_session_run_hooks.py:260] loss = 0.9531218, step = 50 (4.524 sec)\n",
            "I0723 13:19:12.722784 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.21319\n",
            "I0723 13:19:12.729321 140336914397056 basic_session_run_hooks.py:260] loss = 0.7561656, step = 60 (4.521 sec)\n",
            "I0723 13:19:17.214735 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.22608\n",
            "I0723 13:19:17.218873 140336914397056 basic_session_run_hooks.py:260] loss = 0.49873465, step = 70 (4.491 sec)\n",
            "I0723 13:19:21.684898 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.23706\n",
            "I0723 13:19:21.692801 140336914397056 basic_session_run_hooks.py:260] loss = 0.39891768, step = 80 (4.474 sec)\n",
            "I0723 13:19:26.136451 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.24641\n",
            "I0723 13:19:26.138781 140336914397056 basic_session_run_hooks.py:260] loss = 0.5090199, step = 90 (4.446 sec)\n",
            "I0723 13:19:30.564652 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.25825\n",
            "I0723 13:19:30.570413 140336914397056 basic_session_run_hooks.py:260] loss = 0.5188272, step = 100 (4.432 sec)\n",
            "I0723 13:19:34.991943 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.25872\n",
            "I0723 13:19:34.994397 140336914397056 basic_session_run_hooks.py:260] loss = 0.35368162, step = 110 (4.424 sec)\n",
            "I0723 13:19:39.396376 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.27044\n",
            "I0723 13:19:39.401321 140336914397056 basic_session_run_hooks.py:260] loss = 0.40367836, step = 120 (4.407 sec)\n",
            "I0723 13:19:43.791784 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.2751\n",
            "I0723 13:19:43.797484 140336914397056 basic_session_run_hooks.py:260] loss = 0.434394, step = 130 (4.396 sec)\n",
            "I0723 13:19:48.175620 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.2811\n",
            "I0723 13:19:48.180310 140336914397056 basic_session_run_hooks.py:260] loss = 0.27118707, step = 140 (4.383 sec)\n",
            "I0723 13:19:52.558214 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.28177\n",
            "I0723 13:19:52.563565 140336914397056 basic_session_run_hooks.py:260] loss = 0.4713543, step = 150 (4.383 sec)\n",
            "I0723 13:19:56.940498 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.28191\n",
            "I0723 13:19:56.945040 140336914397056 basic_session_run_hooks.py:260] loss = 0.42256328, step = 160 (4.381 sec)\n",
            "I0723 13:20:01.325343 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.2806\n",
            "I0723 13:20:01.331622 140336914397056 basic_session_run_hooks.py:260] loss = 0.19502226, step = 170 (4.386 sec)\n",
            "I0723 13:20:05.707216 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.2821\n",
            "I0723 13:20:05.713235 140336914397056 basic_session_run_hooks.py:260] loss = 0.43273365, step = 180 (4.382 sec)\n",
            "I0723 13:20:10.092134 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.28056\n",
            "I0723 13:20:10.097855 140336914397056 basic_session_run_hooks.py:260] loss = 0.40416202, step = 190 (4.385 sec)\n",
            "I0723 13:20:14.483207 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.27734\n",
            "I0723 13:20:14.488750 140336914397056 basic_session_run_hooks.py:260] loss = 0.6255391, step = 200 (4.391 sec)\n",
            "I0723 13:20:18.874258 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.27736\n",
            "I0723 13:20:18.876484 140336914397056 basic_session_run_hooks.py:260] loss = 0.5674994, step = 210 (4.388 sec)\n",
            "I0723 13:20:23.266295 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.27684\n",
            "I0723 13:20:23.268552 140336914397056 basic_session_run_hooks.py:260] loss = 0.55555046, step = 220 (4.392 sec)\n",
            "I0723 13:20:27.676591 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.26744\n",
            "I0723 13:20:27.682554 140336914397056 basic_session_run_hooks.py:260] loss = 0.19219688, step = 230 (4.414 sec)\n",
            "I0723 13:20:32.091654 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.26498\n",
            "I0723 13:20:32.098592 140336914397056 basic_session_run_hooks.py:260] loss = 0.13249107, step = 240 (4.416 sec)\n",
            "I0723 13:20:36.517329 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.25953\n",
            "I0723 13:20:36.522249 140336914397056 basic_session_run_hooks.py:260] loss = 0.36306202, step = 250 (4.424 sec)\n",
            "I0723 13:20:40.939093 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.26155\n",
            "I0723 13:20:40.945302 140336914397056 basic_session_run_hooks.py:260] loss = 0.16940683, step = 260 (4.423 sec)\n",
            "I0723 13:20:45.364282 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.25978\n",
            "I0723 13:20:45.370608 140336914397056 basic_session_run_hooks.py:260] loss = 0.36302784, step = 270 (4.425 sec)\n",
            "I0723 13:20:49.788845 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.26011\n",
            "I0723 13:20:49.793316 140336914397056 basic_session_run_hooks.py:260] loss = 0.094344586, step = 280 (4.423 sec)\n",
            "I0723 13:20:54.219042 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.25724\n",
            "I0723 13:20:54.225119 140336914397056 basic_session_run_hooks.py:260] loss = 0.49516463, step = 290 (4.432 sec)\n",
            "I0723 13:20:58.645176 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.25932\n",
            "I0723 13:20:58.649273 140336914397056 basic_session_run_hooks.py:260] loss = 0.7298648, step = 300 (4.424 sec)\n",
            "I0723 13:21:03.066403 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.26179\n",
            "I0723 13:21:03.072062 140336914397056 basic_session_run_hooks.py:260] loss = 0.31313998, step = 310 (4.423 sec)\n",
            "I0723 13:21:07.478382 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.26656\n",
            "I0723 13:21:07.483526 140336914397056 basic_session_run_hooks.py:260] loss = 0.31327146, step = 320 (4.411 sec)\n",
            "I0723 13:21:11.894846 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.26426\n",
            "I0723 13:21:11.907223 140336914397056 basic_session_run_hooks.py:260] loss = 0.11593647, step = 330 (4.424 sec)\n",
            "I0723 13:21:16.313632 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.26307\n",
            "I0723 13:21:16.319792 140336914397056 basic_session_run_hooks.py:260] loss = 0.1769068, step = 340 (4.413 sec)\n",
            "I0723 13:21:20.722192 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.26832\n",
            "I0723 13:21:20.725405 140336914397056 basic_session_run_hooks.py:260] loss = 0.14216448, step = 350 (4.406 sec)\n",
            "I0723 13:21:25.122826 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.27239\n",
            "I0723 13:21:25.127899 140336914397056 basic_session_run_hooks.py:260] loss = 0.24205068, step = 360 (4.402 sec)\n",
            "I0723 13:21:29.523846 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.2722\n",
            "I0723 13:21:29.526303 140336914397056 basic_session_run_hooks.py:260] loss = 0.08283702, step = 370 (4.398 sec)\n",
            "I0723 13:21:33.928416 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.27037\n",
            "I0723 13:21:33.934264 140336914397056 basic_session_run_hooks.py:260] loss = 0.10631463, step = 380 (4.408 sec)\n",
            "I0723 13:21:38.339562 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.26698\n",
            "I0723 13:21:38.346215 140336914397056 basic_session_run_hooks.py:260] loss = 0.30929637, step = 390 (4.412 sec)\n",
            "I0723 13:21:42.748931 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.2679\n",
            "I0723 13:21:42.757453 140336914397056 basic_session_run_hooks.py:260] loss = 0.31453684, step = 400 (4.411 sec)\n",
            "I0723 13:21:47.164623 140336914397056 basic_session_run_hooks.py:692] global_step/sec: 2.26465\n",
            "I0723 13:21:47.169471 140336914397056 basic_session_run_hooks.py:260] loss = 0.3929595, step = 410 (4.412 sec)\n",
            "I0723 13:21:50.698074 140336914397056 basic_session_run_hooks.py:606] Saving checkpoints for 419 into log/model.ckpt.\n",
            "I0723 13:22:05.809162 140336914397056 estimator.py:368] Loss for final step: 0.36629516.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYRHXA2cdKyk",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:15:45.309668Z",
          "start_time": "2019-05-10T03:00:04.197Z"
        },
        "id": "9m6A-f2V2gdL",
        "colab_type": "code",
        "outputId": "34e7be0a-42d1-430e-98f4-49680fb3c2d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "if do_eval:\n",
        "    eval_examples = processor.get_dev_examples()\n",
        "    num_actual_eval_examples = len(eval_examples)\n",
        "    eval_file = os.path.join(log_dir, \"eval.tf_record\")\n",
        "    file_based_convert_examples_to_features(\n",
        "        eval_examples, label_map, max_seq_length, tokenizer, eval_file)\n",
        "    tf.logging.info(\"***** Running evaluation *****\")\n",
        "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
        "                    len(eval_examples), num_actual_eval_examples,\n",
        "                    len(eval_examples) - num_actual_eval_examples)\n",
        "    tf.logging.info(\"  Batch size = %d\", eval_batch_size)\n",
        "\n",
        "    eval_input_fn = file_based_input_fn_builder(\n",
        "        input_file=eval_file,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=False,\n",
        "        drop_remainder=False,\n",
        "        batch_size=eval_batch_size)\n",
        "\n",
        "    result = estimator.evaluate(input_fn=eval_input_fn)\n",
        "\n",
        "    output_eval_file = os.path.join(log_dir, \"eval_results.txt\")\n",
        "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "        tf.logging.info(\"***** Eval results *****\")\n",
        "        for key in sorted(result.keys()):\n",
        "            tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0723 13:22:05.850444 140336914397056 <ipython-input-10-4818e34a2ae0>:9] Writing example 0 of 500\n",
            "I0723 13:22:05.852176 140336914397056 <ipython-input-9-319d0dfaaa64>:63] *** Example ***\n",
            "I0723 13:22:05.853132 140336914397056 <ipython-input-9-319d0dfaaa64>:64] guid: dev-0\n",
            "I0723 13:22:05.854054 140336914397056 <ipython-input-9-319d0dfaaa64>:66] tokens: [CLS] i want to fly from bost ##on at 838 am and arrive in den ##ver at 1110 in the morning [SEP]\n",
            "I0723 13:22:05.854931 140336914397056 <ipython-input-9-319d0dfaaa64>:68] input_ids: 101 177 21528 10114 26155 10188 29495 10263 10160 82665 10392 10111 27814 10106 10140 12563 10160 106270 10106 10105 28757 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:22:05.855885 140336914397056 <ipython-input-9-319d0dfaaa64>:70] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:22:05.856631 140336914397056 <ipython-input-9-319d0dfaaa64>:72] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:22:05.859404 140336914397056 <ipython-input-9-319d0dfaaa64>:73] label: atis_flight (id = 11)\n",
            "I0723 13:22:05.861112 140336914397056 <ipython-input-9-319d0dfaaa64>:63] *** Example ***\n",
            "I0723 13:22:05.862195 140336914397056 <ipython-input-9-319d0dfaaa64>:64] guid: dev-1\n",
            "I0723 13:22:05.863291 140336914397056 <ipython-input-9-319d0dfaaa64>:66] tokens: [CLS] show me all round trip flights between hou ##ston and las veg ##as [SEP]\n",
            "I0723 13:22:05.864339 140336914397056 <ipython-input-9-319d0dfaaa64>:68] input_ids: 101 11897 10911 10435 13569 37307 55650 10948 109601 21884 10111 10285 108193 10403 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:22:05.865271 140336914397056 <ipython-input-9-319d0dfaaa64>:70] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:22:05.866322 140336914397056 <ipython-input-9-319d0dfaaa64>:72] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:22:05.867287 140336914397056 <ipython-input-9-319d0dfaaa64>:73] label: atis_flight (id = 11)\n",
            "I0723 13:22:06.110024 140336914397056 <ipython-input-16-2cefe2c006b1>:7] ***** Running evaluation *****\n",
            "I0723 13:22:06.111148 140336914397056 <ipython-input-16-2cefe2c006b1>:10]   Num examples = 500 (500 actual, 0 padding)\n",
            "I0723 13:22:06.112102 140336914397056 <ipython-input-16-2cefe2c006b1>:11]   Batch size = 32\n",
            "I0723 13:22:06.154705 140336914397056 estimator.py:1145] Calling model_fn.\n",
            "I0723 13:22:06.155534 140336914397056 <ipython-input-14-64f48bfe5f20>:4] *** Features ***\n",
            "I0723 13:22:06.156512 140336914397056 <ipython-input-14-64f48bfe5f20>:6]   name = input_ids, shape = (?, 50)\n",
            "I0723 13:22:06.157484 140336914397056 <ipython-input-14-64f48bfe5f20>:6]   name = input_mask, shape = (?, 50)\n",
            "I0723 13:22:06.158506 140336914397056 <ipython-input-14-64f48bfe5f20>:6]   name = is_real_example, shape = (?,)\n",
            "I0723 13:22:06.159468 140336914397056 <ipython-input-14-64f48bfe5f20>:6]   name = label_ids, shape = (?,)\n",
            "I0723 13:22:06.160383 140336914397056 <ipython-input-14-64f48bfe5f20>:6]   name = segment_ids, shape = (?, 50)\n",
            "I0723 13:22:09.522019 140336914397056 estimator.py:1147] Done calling model_fn.\n",
            "I0723 13:22:09.551588 140336914397056 evaluation.py:255] Starting evaluation at 2019-07-23T13:22:09Z\n",
            "I0723 13:22:10.048049 140336914397056 monitored_session.py:240] Graph was finalized.\n",
            "W0723 13:22:10.052632 140336914397056 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0723 13:22:10.062968 140336914397056 saver.py:1280] Restoring parameters from log/model.ckpt-419\n",
            "I0723 13:22:10.942102 140336914397056 session_manager.py:500] Running local_init_op.\n",
            "I0723 13:22:11.012599 140336914397056 session_manager.py:502] Done running local_init_op.\n",
            "I0723 13:22:13.673029 140336914397056 evaluation.py:275] Finished evaluation at 2019-07-23-13:22:13\n",
            "I0723 13:22:13.674335 140336914397056 estimator.py:2039] Saving dict for global step 419: accuracy = 0.946, global_step = 419, loss = 0.25578693\n",
            "I0723 13:22:14.194340 140336914397056 estimator.py:2099] Saving 'checkpoint_path' summary for global step 419: log/model.ckpt-419\n",
            "I0723 13:22:14.200903 140336914397056 <ipython-input-16-2cefe2c006b1>:24] ***** Eval results *****\n",
            "I0723 13:22:14.202004 140336914397056 <ipython-input-16-2cefe2c006b1>:26]   accuracy = 0.946\n",
            "I0723 13:22:14.204263 140336914397056 <ipython-input-16-2cefe2c006b1>:26]   global_step = 419\n",
            "I0723 13:22:14.205806 140336914397056 <ipython-input-16-2cefe2c006b1>:26]   loss = 0.25578693\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKalVYoAdOno",
        "colab_type": "text"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-10T03:15:45.310973Z",
          "start_time": "2019-05-10T03:00:04.199Z"
        },
        "id": "8MCEcX892gdM",
        "colab_type": "code",
        "outputId": "46d0dd73-2127-466e-b741-eb8d78536c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "source": [
        "if do_predict:\n",
        "\n",
        "    predict_examples = processor.get_test_examples()\n",
        "    num_actual_predict_examples = len(predict_examples)\n",
        "\n",
        "    predict_file = os.path.join(log_dir, \"predict.tf_record\")\n",
        "    if not tf.gfile.Exists(predict_file):\n",
        "        file_based_convert_examples_to_features(predict_examples, label_map,\n",
        "                                                max_seq_length, tokenizer,\n",
        "                                                predict_file)\n",
        "\n",
        "    tf.logging.info(\"***** Running prediction*****\")\n",
        "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
        "                    len(predict_examples), num_actual_predict_examples,\n",
        "                    len(predict_examples) - num_actual_predict_examples)\n",
        "    tf.logging.info(\"  Batch size = %d\", predict_batch_size)\n",
        "\n",
        "    predict_input_fn = file_based_input_fn_builder(\n",
        "        input_file=predict_file,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=False,\n",
        "        drop_remainder=False,\n",
        "        batch_size=predict_batch_size)\n",
        "\n",
        "    result = estimator.predict(input_fn=predict_input_fn)\n",
        "    label_map_new = {v: k for k, v in label_map.items()}\n",
        "\n",
        "\n",
        "    output_predict_file = os.path.join(log_dir, \"test_results.tsv\")\n",
        "    with tf.gfile.GFile(output_predict_file, \"w\") as writer:\n",
        "        writer.write(\"line, true, predict \\n\")\n",
        "        tf.logging.info(\"***** Predict results *****\")\n",
        "\n",
        "        true_classes = []\n",
        "        predicted_classes = []\n",
        "\n",
        "        for i, item in enumerate(result):\n",
        "                true_class = item[\"true_class\"]\n",
        "                predicted_class = item[\"predicted_class\"]\n",
        "\n",
        "                true_classes.append(true_class)\n",
        "                predicted_classes.append(predicted_class)\n",
        "\n",
        "                if predicted_class != true_class:\n",
        "                    output_line = \"{}, {}, {}\\n\".format(i + 1, label_map_new[true_class],\n",
        "                                                        label_map_new[predicted_class])\n",
        "                    writer.write(output_line)\n",
        "    accuracy = metrics.accuracy_score(true_classes, predicted_classes)\n",
        "#     classification_report = metrics.classification_report(true_classes, predicted_classes)\n",
        "    tf.logging.info(\"Accuracy: %s\", accuracy)\n",
        "#     tf.logging.info(\"Accuracy: %s\", classification_report)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0723 13:22:14.252869 140336914397056 <ipython-input-10-4818e34a2ae0>:9] Writing example 0 of 893\n",
            "I0723 13:22:14.254212 140336914397056 <ipython-input-9-319d0dfaaa64>:63] *** Example ***\n",
            "I0723 13:22:14.255013 140336914397056 <ipython-input-9-319d0dfaaa64>:64] guid: test-0\n",
            "I0723 13:22:14.256021 140336914397056 <ipython-input-9-319d0dfaaa64>:66] tokens: [CLS] i would like to find a flight from char ##lotte to las veg ##as that makes a stop in st . lo ##uis [SEP]\n",
            "I0723 13:22:14.257133 140336914397056 <ipython-input-9-319d0dfaaa64>:68] input_ids: 101 177 10894 11850 10114 17860 169 23578 10188 101328 92431 10114 10285 108193 10403 10189 20562 169 20517 10106 28780 119 10406 31466 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:22:14.258118 140336914397056 <ipython-input-9-319d0dfaaa64>:70] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:22:14.259314 140336914397056 <ipython-input-9-319d0dfaaa64>:72] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:22:14.260582 140336914397056 <ipython-input-9-319d0dfaaa64>:73] label: atis_flight (id = 11)\n",
            "I0723 13:22:14.262749 140336914397056 <ipython-input-9-319d0dfaaa64>:63] *** Example ***\n",
            "I0723 13:22:14.263705 140336914397056 <ipython-input-9-319d0dfaaa64>:64] guid: test-1\n",
            "I0723 13:22:14.265412 140336914397056 <ipython-input-9-319d0dfaaa64>:66] tokens: [CLS] on april first i need a ticket from ta ##coma to san jos ##e de ##parti ##ng before 7 am [SEP]\n",
            "I0723 13:22:14.266559 140336914397056 <ipython-input-9-319d0dfaaa64>:68] input_ids: 101 10135 11874 10422 177 17367 169 59037 10188 11057 76454 10114 14608 21705 10112 10104 96503 10376 11360 128 10392 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:22:14.273292 140336914397056 <ipython-input-9-319d0dfaaa64>:70] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:22:14.281083 140336914397056 <ipython-input-9-319d0dfaaa64>:72] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0723 13:22:14.282471 140336914397056 <ipython-input-9-319d0dfaaa64>:73] label: atis_airfare (id = 22)\n",
            "I0723 13:22:14.620720 140336914397056 <ipython-input-17-888b15f849c3>:12] ***** Running prediction*****\n",
            "I0723 13:22:14.621891 140336914397056 <ipython-input-17-888b15f849c3>:15]   Num examples = 893 (893 actual, 0 padding)\n",
            "I0723 13:22:14.622970 140336914397056 <ipython-input-17-888b15f849c3>:16]   Batch size = 32\n",
            "I0723 13:22:14.624370 140336914397056 <ipython-input-17-888b15f849c3>:32] ***** Predict results *****\n",
            "I0723 13:22:14.668606 140336914397056 estimator.py:1145] Calling model_fn.\n",
            "I0723 13:22:14.669457 140336914397056 <ipython-input-14-64f48bfe5f20>:4] *** Features ***\n",
            "I0723 13:22:14.670364 140336914397056 <ipython-input-14-64f48bfe5f20>:6]   name = input_ids, shape = (?, 50)\n",
            "I0723 13:22:14.671592 140336914397056 <ipython-input-14-64f48bfe5f20>:6]   name = input_mask, shape = (?, 50)\n",
            "I0723 13:22:14.672676 140336914397056 <ipython-input-14-64f48bfe5f20>:6]   name = is_real_example, shape = (?,)\n",
            "I0723 13:22:14.674077 140336914397056 <ipython-input-14-64f48bfe5f20>:6]   name = label_ids, shape = (?,)\n",
            "I0723 13:22:14.675306 140336914397056 <ipython-input-14-64f48bfe5f20>:6]   name = segment_ids, shape = (?, 50)\n",
            "I0723 13:22:17.792460 140336914397056 estimator.py:1147] Done calling model_fn.\n",
            "I0723 13:22:18.460734 140336914397056 monitored_session.py:240] Graph was finalized.\n",
            "I0723 13:22:18.472375 140336914397056 saver.py:1280] Restoring parameters from log/model.ckpt-419\n",
            "I0723 13:22:19.312821 140336914397056 session_manager.py:500] Running local_init_op.\n",
            "I0723 13:22:19.364040 140336914397056 session_manager.py:502] Done running local_init_op.\n",
            "I0723 13:22:23.221600 140336914397056 <ipython-input-17-888b15f849c3>:50] Accuracy: 0.8801791713325868\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}